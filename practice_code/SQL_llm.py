import gradio as gr
import pandas as pd
import hashlib
import os
import re
import io
import numpy as np
import pdfplumber
import pytesseract
from PIL import Image
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.document_loaders import PyMuPDFLoader
import camelot
import ollama
import sqlite3
from datetime import datetime
import json

# Cache for retrievers and DataFrames
retriever_cache = {}
dataframe_cache = {}

# Embedding model
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

QA_LOG_FILE = "qa_data.json" 

def save_qa_to_json(user_question, assistant_answer, system_prompt="ë‹¹ì‹ ì€ AI ê¸°ì—… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•´ì£¼ëŠ” ì§€ì‹ ë¹„ì„œì…ë‹ˆë‹¤."):
    """QA ë¡œê·¸ë¥¼ JSON íŒŒì¼ì— ì €ì¥"""
    qa_entry = {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question},
            {"role": "assistant", "content": assistant_answer}
        ]
    }

    if os.path.exists(QA_LOG_FILE):
        with open(QA_LOG_FILE, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = []
    else:
        data = []

    data.append(qa_entry)

    with open(QA_LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ QA ì €ì¥ ì™„ë£Œ: {user_question}")

def get_file_hash(file):
    """íŒŒì¼ í•´ì‹œ ìƒì„±"""
    try:
        file_path = file.name if hasattr(file, "name") else file
        with open(file_path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] í•´ì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def is_sql_query(query):
    """SQL ì¿¼ë¦¬ íŒ¨í„´ ê°ì§€"""
    sql_patterns = [
        r'select\s+.*\s+from',
        r'count\(\*\)',
        r'where\s+.*\s+like',
        r'group\s+by',
        r'order\s+by',
        r'ë¶€ì‚°.*ëª‡\s*ê°œ',
        r'ì„œìš¸.*ëª‡\s*ê°œ',
        r'ê´‘ì£¼.*ëª‡\s*ê°œ',
        r'ê²½ìƒ.*ëª‡\s*ê°œ',
        r'ê²½ë‚¨.*ëª‡\s*ê°œ',
        r'ê²½ë¶.*ëª‡\s*ê°œ',
        r'ì¸ì²œ.*ëª‡\s*ê°œ',
        r'ì œì£¼.*ëª‡\s*ê°œ',
        r'ê°•ì›.*ëª‡\s*ê°œ',
        r'ëŒ€ì „.*ëª‡\s*ê°œ',
        r'ê²½ê¸°.*ëª‡\s*ê°œ',
        r'ì¶©ë‚¨.*ëª‡\s*ê°œ',
        r'ì¶©ë¶.*ëª‡\s*ê°œ',
        r'ì¶©ì²­.*ëª‡\s*ê°œ',
        r'ëŒ€êµ¬.*ëª‡\s*ê°œ',
        r'ì „ë¼.*ëª‡\s*ê°œ',
        r'ì „ë‚¨.*ëª‡\s*ê°œ',
        r'ì „ë¶.*ëª‡\s*ê°œ',
        r'ìš¸ì‚°.*ëª‡\s*ê°œ',
        r'.*ì§€ì—­.*ê°œìˆ˜',
        r'.*ì§€ì—­.*ëª‡',
        r'ì‹œê°ì§€ëŠ¥.*ëª‡',
        r'ë¶„ì„ì§€ëŠ¥.*ëª‡',
        r'ì–¸ì–´.*ìŒì„±.*ëª‡',
        r'í–‰ë™ì§€ëŠ¥.*ëª‡',
        r'ì „ì²´.*ëª‡',
        r'ì´.*ê°œ',
        r'ì„¤ë¦½ì—°ë„.*ëª‡'
    ]
    query_lower = query.lower()
    return any(re.search(pattern, query_lower) for pattern in sql_patterns)

def convert_to_sql(query):
    """ìì—°ì–´ë¥¼ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜"""
    query_lower = query.lower()
    
    # ì§€ì—­ë³„ ê¸°ì—… ìˆ˜ ì¿¼ë¦¬
    regions = {
        'ë¶€ì‚°': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ë¶€ì‚°%'",
        'ì„œìš¸': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì„œìš¸%'",
        'ê´‘ì£¼': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê´‘ì£¼%'",
        'ì¸ì²œ': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¸ì²œ%'",
        'ëŒ€ì „': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ëŒ€ì „%'",
        'ëŒ€êµ¬': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ëŒ€êµ¬%'",
        'ìš¸ì‚°': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ìš¸ì‚°%'",
        'ê²½ê¸°': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ê¸°%'",
        'ê°•ì›': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê°•ì›%'",
        'ì œì£¼': "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì œì£¼%'",
    }
    
    for region, sql in regions.items():
        if region in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
            return sql
    
    # ê²½ìƒë„ ê´€ë ¨
    if ('ê²½ë‚¨' in query or 'ê²½ìƒë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ìƒë‚¨ë„%'"
    
    if ('ê²½ë¶' in query or 'ê²½ìƒë¶ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ìƒë¶ë„%'"
    
    if ('ê²½ìƒ' in query or 'ê²½ìƒë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ê²½ìƒë‚¨ë„%' OR ì§€ì—­ LIKE '%ê²½ìƒë¶ë„%')"
    
    # ì¶©ì²­ë„ ê´€ë ¨
    if ('ì¶©ë‚¨' in query or 'ì¶©ì²­ë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¶©ì²­ë‚¨ë„%'"
    
    if ('ì¶©ë¶' in query or 'ì¶©ì²­ë¶ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¶©ì²­ë¶ë„%'"
    
    if 'ì¶©ì²­' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ì¶©ì²­ë‚¨ë„%' OR ì§€ì—­ LIKE '%ì¶©ì²­ë¶ë„%')"
    
    # ì „ë¼ë„ ê´€ë ¨
    if ('ì „ë‚¨' in query or 'ì „ë¼ë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì „ë¼ë‚¨ë„%'"
    
    if ('ì „ë¶' in query or 'ì „ë¼ë¶ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì „ë¼ë¶ë„%'"
    
    if 'ì „ë¼ë„' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ì „ë¼ë‚¨ë„%' OR ì§€ì—­ LIKE '%ì „ë¼ë¶ë„%')"
    
    # ì „ë¬¸ë¶„ì•¼ë³„ ê°œìˆ˜
    if 'ì‹œê°ì§€ëŠ¥' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì „ë¬¸ë¶„ì•¼ LIKE '%ì‹œê°ì§€ëŠ¥%'"
    
    if 'ë¶„ì„ì§€ëŠ¥' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì „ë¬¸ë¶„ì•¼ LIKE '%ë¶„ì„ì§€ëŠ¥%'"
    
    if ('ì–¸ì–´Â·ìŒì„±ì§€ëŠ¥' in query or 'ì–¸ì–´,ìŒì„±ì§€ëŠ¥' in query or 'ì–¸ì–´ìŒì„±ì§€ëŠ¥' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì „ë¬¸ë¶„ì•¼ LIKE '%ì–¸ì–´Â·ìŒì„±ì§€ëŠ¥%'"
    
    if 'í–‰ë™ì§€ëŠ¥' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì „ë¬¸ë¶„ì•¼ LIKE '%í–‰ë™ì§€ëŠ¥%'"
    
    # ì„¤ë¦½ì—°ë„ë³„ ë¶„ì„
    if 'ë¶€ì‚°' in query and ('ì„¤ë¦½ì—°ë„' in query or 'ì—°ë„ë³„' in query):
        return "SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, COUNT(*) AS ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ë¶€ì‚°%' GROUP BY ì„¤ë¦½ì—°ë„ ORDER BY ì„¤ë¦½ì—°ë„"
    
    if 'ì„œìš¸' in query and ('ì„¤ë¦½ì—°ë„' in query or 'ì—°ë„ë³„' in query):
        return "SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, COUNT(*) AS ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì„œìš¸%' GROUP BY ì„¤ë¦½ì—°ë„ ORDER BY ì„¤ë¦½ì—°ë„"
    
    if ('ì „ì²´' in query or 'ëª¨ë“ ' in query) and ('ì„¤ë¦½ì—°ë„' in query or 'ì—°ë„ë³„' in query):
        return "SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, COUNT(*) AS ê¸°ì—…ìˆ˜ FROM companies GROUP BY ì„¤ë¦½ì—°ë„ ORDER BY ì„¤ë¦½ì—°ë„"
    
    # ì „ì²´ ê¸°ì—… ìˆ˜
    if ('ì „ì²´' in query or 'ì´' in query or 'ëª¨ë“ ' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ëª… FROM companies"
    
    return None

def create_sqlite_from_dataframe(df, db_path="temp_companies.db"):
    """DataFrameì„ SQLite DBë¡œ ë³€í™˜"""
    try:
        conn = sqlite3.connect(db_path)
        df.to_sql('companies', conn, if_exists='replace', index=False)
        conn.close()
        return True
    except Exception as e:
        print(f"[ERROR] SQLite DB ìƒì„± ì‹¤íŒ¨: {e}")
        return False

def execute_sql_query(sql_query, db_path="temp_companies.db"):
    """SQL ì¿¼ë¦¬ ì‹¤í–‰"""
    try:
        conn = sqlite3.connect(db_path)
        result = pd.read_sql_query(sql_query, conn)
        conn.close()
        return result
    except Exception as e:
        print(f"[ERROR] SQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

def extract_tables_from_pdf(file_path, save_csv_path="auto_extracted_table.csv"):
    """PDFì—ì„œ í…Œì´ë¸” ì¶”ì¶œ"""
    try:
        tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
        if len(tables) == 0:
            tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
        
        table_docs = []
        all_dfs = []
        
        for i, table in enumerate(tables):
            df = table.df
            all_dfs.append(df)
            
            if len(df) > 0:
                header = [str(col).strip() for col in df.iloc[0]]
                df.columns = header
                df = df.iloc[1:].reset_index(drop=True)
                
                for idx, row in df.iterrows():
                    row_data = [str(cell).strip() for cell in row]
                    row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header)) if i < len(row_data)])
                    table_docs.append(Document(page_content=f"[í…Œì´ë¸” {i+1}, í–‰ {idx+1}]\n{row_text}"))
        
        if all_dfs:
            main_df = all_dfs[0].copy()
            if len(main_df) > 0:
                header = [str(col).strip() for col in main_df.iloc[0]]
                main_df.columns = header
                main_df = main_df.iloc[1:].reset_index(drop=True)
                
                for df in all_dfs[1:]:
                    if len(df) > 0:
                        df.columns = header[:len(df.columns)]
                        df = df.iloc[1:].reset_index(drop=True)
                        main_df = pd.concat([main_df, df], ignore_index=True)
                
                main_df.to_csv(save_csv_path, index=False, encoding='utf-8-sig')
                print(f"âœ… CSVë¡œ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {save_csv_path}")
                
                dataframe_cache['companies'] = main_df
                create_sqlite_from_dataframe(main_df)
                
        return table_docs
    except Exception as e:
        print(f"[ERROR] í…Œì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def extract_text_from_pdf(file_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()
        
        if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
            print("ğŸ“„ OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...")
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 100:
                        img = page.to_image()
                        text = pytesseract.image_to_string(img, lang='kor+eng')
                    if text:
                        docs.append(Document(page_content=f"[í˜ì´ì§€ {page_num+1}]\n{text}"))
        
        table_docs = extract_tables_from_pdf(file_path)
        docs.extend(table_docs)
        
        return docs
    except Exception as e:
        print(f"[ERROR] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def split_text(docs):
    """í…ìŠ¤íŠ¸ ë¶„í• """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " "]
    )
    
    table_docs = [doc for doc in docs if doc.page_content.startswith("[í…Œì´ë¸”")]
    text_docs = [doc for doc in docs if not doc.page_content.startswith("[í…Œì´ë¸”")]
    
    split_text_docs = splitter.split_documents(text_docs)
    return split_text_docs + table_docs

def format_context(retrieved_docs, max_length=3000):
    """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    return context[:max_length]

def is_count_question(query):
    """ê°œìˆ˜ ì§ˆë¬¸ íŒ¨í„´ ê°ì§€"""
    patterns = [
        r'ê°œìˆ˜', r'ëª‡\s*ê°œ', r'ëª‡\s*ëª…', r'ì´\s*ê°œ', r'ì´\s*ìˆ˜', 
        r'ì´\s*ì¸ì›', r'ìˆ˜ëŠ”', r'ê°œëŠ”', r'ëª‡\s*ê³³', r'ëª‡\s*íšŒ', 
        r'ì–¼ë§ˆë‚˜', r'ë§ì€', r'ìˆëŠ”', r'ìˆìŠµë‹ˆê¹Œ'
    ]
    return any(re.search(pattern, query) for pattern in patterns)

def format_sql_result_for_llm(sql_result, query):
    """SQL ê²°ê³¼ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ í¬ë§·íŒ…"""
    if sql_result is None or len(sql_result) == 0:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    # ë‹¨ì¼ ê°œìˆ˜ ê²°ê³¼
    if len(sql_result.columns) == 1 and len(sql_result) == 1:
        count = sql_result.iloc[0, 0]
        return f"ê²€ìƒ‰ ê²°ê³¼: {count}ê°œ"
    
    # ì—°ë„ë³„ ë˜ëŠ” ê·¸ë£¹ë³„ ê²°ê³¼
    if len(sql_result.columns) == 2:
        result_text = "ê²€ìƒ‰ ê²°ê³¼:\n"
        for _, row in sql_result.iterrows():
            result_text += f"- {row.iloc[0]}: {row.iloc[1]}ê°œ\n"
        total = sql_result.iloc[:, 1].sum()
        result_text += f"ì´í•©: {total}ê°œ"
        return result_text
    
    return sql_result.to_string()

class Generator:
    def __call__(self, query, context=None, sql_result=None):
        """LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
        system_prompt = "ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        prompt = f"ì§ˆë¬¸: {query}\n\n"
        
        # SQL ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ìš°ì„  ì‚¬ìš©
        if sql_result is not None:
            formatted_result = format_sql_result_for_llm(sql_result, query)
            prompt += f"ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼:\n{formatted_result}\n\n"
            prompt += "ìœ„ì˜ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
        elif context:
            prompt += f"ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:\n{context}\n\n"
            prompt += "ìœ„ì˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì„¸ìš”."
        
        try:
            response = ollama.chat(
                model='benedict/linkbricks-llama3.1-korean:8b ',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            return response['message']['content']
        except Exception as e:
            # SQL ê²°ê³¼ê°€ ìˆìœ¼ë©´ ê°„ë‹¨í•œ í…œí”Œë¦¿ ë‹µë³€
            if sql_result is not None:
                if len(sql_result) == 1 and len(sql_result.columns) == 1:
                    count = sql_result.iloc[0, 0]
                    return f"ê²€ìƒ‰í•˜ì‹  ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ê¸°ì—… ìˆ˜ëŠ” **{count}ê°œ**ì…ë‹ˆë‹¤."
                else:
                    return f"ê²€ìƒ‰ ê²°ê³¼:\n{sql_result.to_string()}"
            
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file):
        """ë©”ì¸ RAG íŒŒì´í”„ë¼ì¸"""
        if not file:
            return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
        
        file_hash = get_file_hash(file)
        
        # ë¨¼ì € ë°ì´í„° ì¶”ì¶œ (ìºì‹œ í™•ì¸)
        if 'companies' not in dataframe_cache:
            print("ğŸ“Š PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            docs = extract_text_from_pdf(file.name)
        
        # SQL ì¿¼ë¦¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œì§€ í™•ì¸
        if is_sql_query(query):
            print("ğŸ” SQL ì¿¼ë¦¬ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•œ ì§ˆë¬¸ ê°ì§€")
            
            sql_query = convert_to_sql(query)
            if sql_query:
                print(f"ğŸ“ ìƒì„±ëœ SQL: {sql_query}")
                sql_result = execute_sql_query(sql_query)
                
                if sql_result is not None:
                    print(f"âœ… SQL ê²°ê³¼ íšë“")
                    # SQL ê²°ê³¼ë¥¼ LLMìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€ ìƒì„±
                    response = self.generator(query, sql_result=sql_result)
                    return response
                else:
                    print("âŒ SQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨, RAG ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
        
        # ì¼ë°˜ RAG ê²€ìƒ‰
        print("ğŸ” ì¼ë°˜ RAG ê²€ìƒ‰ ì‹¤í–‰")
        
        if file_hash in retriever_cache:
            print("ğŸ“¦ ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
            vectorstore = retriever_cache[file_hash]
        else:
            print("ğŸ”„ ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            if 'companies' not in dataframe_cache:
                docs = extract_text_from_pdf(file.name)
            else:
                docs = extract_text_from_pdf(file.name)
            
            split_docs = split_text(docs)
            vectorstore = Chroma.from_documents(split_docs, embeddings)
            retriever_cache[file_hash] = vectorstore

        retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=7, fetch_k=20)
        
        if is_count_question(query):
            table_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" in doc.page_content]
            other_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" not in doc.page_content]
            retrieved_docs = table_docs + other_docs

        context = format_context(retrieved_docs)
        return self.generator(query, context=context)

# Gradio ì¸í„°í˜ì´ìŠ¤
generator = Generator()
rag_pipeline = RAGPipeline(generator)

def chat_interface(message, history, file):
    """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜"""
    try:
        response = rag_pipeline(message, file)
        
        # QA ë¡œê·¸ ì €ì¥
        save_qa_to_json(message, response)
        
        return response
    except Exception as e:
        error_msg = f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        print(f"[ERROR] {error_msg}")
        return error_msg

# Gradio ì• í”Œë¦¬ì¼€ì´ì…˜
chatbot = gr.ChatInterface(
    fn=chat_interface,
    title="ğŸ¤– í†µí•© PDF RAG+SQL ë¶„ì„ ì‹œìŠ¤í…œ",
    description="""
    ğŸ“Š **PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”!**
    
    ğŸ’¡ **ì§€ì›í•˜ëŠ” ì§ˆë¬¸ ìœ í˜•:**
    - ğŸ¢ **ì§€ì—­ë³„ ê¸°ì—… ìˆ˜**: "ë¶€ì‚°ì— ìˆëŠ” ê¸°ì—…ì´ ëª‡ ê°œì•¼?", "ì„œìš¸ ê¸°ì—… ìˆ˜ëŠ”?"
    - ğŸ¯ **ì „ë¬¸ë¶„ì•¼ë³„ ë¶„ì„**: "ì‹œê°ì§€ëŠ¥ ë¶„ì•¼ ê¸°ì—…ì´ ëª‡ ê°œì•¼?", "ë¶„ì„ì§€ëŠ¥ ê¸°ì—… ìˆ˜ëŠ”?"
    - ğŸ“… **ì—°ë„ë³„ ë¶„ì„**: "ë¶€ì‚° ê¸°ì—…ë“¤ì˜ ì„¤ë¦½ì—°ë„ë³„ ë¶„í¬ëŠ”?", "ì „ì²´ ì„¤ë¦½ì—°ë„ë³„ í˜„í™©ì€?"
    - ğŸ“ˆ **ì „ì²´ í†µê³„**: "ì „ì²´ ê¸°ì—… ìˆ˜ëŠ”?", "ì´ ëª‡ ê°œ ê¸°ì—…ì´ ìˆì–´?"
    - ğŸ’¬ **ì¼ë°˜ ì§ˆë¬¸**: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ììœ ë¡œìš´ ì§ˆë¬¸
    
    âš¡ **íŠ¹ì§•:**
    - êµ¬ì¡°í™”ëœ ë°ì´í„°ëŠ” SQLë¡œ ë¹ ë¥´ê²Œ ì²˜ë¦¬
    - ë³µì¡í•œ ì§ˆë¬¸ì€ AI ê²€ìƒ‰ìœ¼ë¡œ ëŒ€ì‘
    - ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë‹µë³€ ì œê³µ
    """,
    additional_inputs=[
        gr.File(
            label="ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ", 
            file_types=[".pdf"],
            type="filepath"
        )
    ],
    theme="soft"
)

chatbot.launch()

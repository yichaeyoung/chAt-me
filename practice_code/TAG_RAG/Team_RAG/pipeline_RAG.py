import gradio as gr
import pandas as pd
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
import ollama
import pdfplumber
import pytesseract
from PIL import Image
import hashlib
import os
import camelot
from langchain.document_loaders import PyMuPDFLoader
import re
import io
import numpy as np

# ìºì‹œ ì €ì¥ì†Œ (LRU ë°©ì‹)
retriever_cache = {}
# CACHE_LIMIT = 5

# íŒŒì¼ í•´ì‹œ ìƒì„± í•¨ìˆ˜
def get_file_hash(file):
    ''' íŒŒì¼ í•´ì‹œ ìƒì„± í•¨ìˆ˜ - íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ None ë°˜í™˜ '''
    try:
        file_path = file.name if hasattr(file, "name") else file
        with open(file_path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    except FileNotFoundError:
        print("[ERROR] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"[ERROR] íŒŒì¼ í•´ì‹œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

'''
í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ í•¨ìˆ˜
camelotì„ ì‚¬ìš©í•˜ì—¬ PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
'''
def extract_tables_from_pdf(file_path):
    try:
        # camelotìœ¼ë¡œ í…Œì´ë¸” ì¶”ì¶œ ì‹œë„
        tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
        
        if len(tables) == 0:
            # latticeë¡œ í…Œì´ë¸”ì´ ë°œê²¬ë˜ì§€ ì•Šìœ¼ë©´ stream ë°©ì‹ ì‹œë„
            tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
        
        table_docs = []
        
        for i, table in enumerate(tables):
            # í…Œì´ë¸”ì„ DataFrameìœ¼ë¡œ ë³€í™˜
            df = table.df
            
            # ì»¬ëŸ¼ëª…ê³¼ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            # ì»¬ëŸ¼ëª…ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° ë° ì •ë¦¬
            header = [col.strip() for col in df.iloc[0]]
            
            # í…Œì´ë¸”ì˜ ê° í–‰ì„ ì²˜ë¦¬
            for idx, row in df.iloc[1:].iterrows():
                row_data = [str(cell).strip() for cell in row]
                # í‚¤-ê°’ ìŒ í˜•íƒœë¡œ í…ìŠ¤íŠ¸ êµ¬ì„± (ì»¬ëŸ¼ëª…: ê°’)
                row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header))])
                
                # í…Œì´ë¸” ë©”íƒ€ë°ì´í„° ì¶”ê°€
                table_info = f"[í…Œì´ë¸” {i+1}, í–‰ {idx}]\n{row_text}"
                table_docs.append(Document(page_content=table_info))
        
        return table_docs
    except Exception as e:
        print(f"[ERROR] í…Œì´ë¸” ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

''' 
PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDFLoader + OCR + í…Œì´ë¸” ì¶”ì¶œ ê²°í•©)
PyMuPDFLoaderë¥¼ ìš°ì„  ì‚¬ìš© í›„ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” í˜ì´ì§€ì—ë§Œ pytesseractë¡œ OCR ì ìš©
ì¶”ê°€ë¡œ camelotì„ ì‚¬ìš©í•˜ì—¬ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ
'''
def extract_text_from_pdf(file_path):
    try:
        # ì¼ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()
        
        # í…ìŠ¤íŠ¸ê°€ ì ê±°ë‚˜ ì—†ëŠ” ê²½ìš° OCR ë³´ì™„
        if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 100:
                        img = page.to_image()
                        text = pytesseract.image_to_string(img, lang='kor+eng')
                    if text:
                        docs.append(Document(page_content=f"[í˜ì´ì§€ {page_num+1}]\n{text}"))
        
        # í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ ë° ì¶”ê°€
        table_docs = extract_tables_from_pdf(file_path)
        docs.extend(table_docs)
        
        return docs
    except Exception as e:
        print(f"[ERROR] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []


''' 
í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜
" ", ". " ë“±ì„ ë„£ì–´ì„œ ë” ì„¸ë°€í•˜ê²Œ ë¶„í• 
í…Œì´ë¸” ë°ì´í„°ëŠ” ë¶„í• í•˜ì§€ ì•Šë„ë¡ ì¡°ì •
'''
def split_text(docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " "]
    )
    
    # í…Œì´ë¸” ë°ì´í„°ì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ ë¶„ë¦¬
    table_docs = [doc for doc in docs if doc.page_content.startswith("[í…Œì´ë¸”")]
    text_docs = [doc for doc in docs if not doc.page_content.startswith("[í…Œì´ë¸”")]
    
    # ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ë¶„í• 
    split_text_docs = text_splitter.split_documents(text_docs)
    
    # í…Œì´ë¸” ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ì—¬ í•©ì¹¨
    return split_text_docs + table_docs

'''
ì»¨í…ìŠ¤íŠ¸ í¬ë§¤íŒ… í•¨ìˆ˜
ì¤‘ìš” ë¬¸ì¥ ìš°ì„  í¬í•¨ ë° ê¸¸ì´ ì œí•œ
'''
def format_context(retrieved_docs, max_length=3000):
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    return context[:max_length]

embeddings = OllamaEmbeddings(model="mxbai-embed-large")

'''
ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ìºì‹œ ê´€ë¦¬
ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ì„œ LRU ë°©ì‹ìœ¼ë¡œ ìºì‹œ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ.(í˜„ì¬ëŠ” ì£¼ì„ì²˜ë¦¬)
'''
def get_vectorstore(docs, file_hash):
    if file_hash in retriever_cache:
        print("ğŸ“¦ ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
        return retriever_cache[file_hash]

    split_docs = split_text(docs)
    vectorstore = Chroma.from_documents(split_docs, embeddings)
    # if len(retriever_cache) >= CACHE_LIMIT:
    #     retriever_cache.pop(next(iter(retriever_cache)))  # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
    retriever_cache[file_hash] = vectorstore
    return vectorstore

'''
ìˆ«ì ì§ˆë¬¸ì— ëŒ€í•œ íŒ¨í„´ ì¸ì‹ í•¨ìˆ˜
ê°œìˆ˜, ëª‡ ê°œ, ëª‡ ëª… ë“±ì˜ íŒ¨í„´ì„ ê°ì§€
'''
def is_count_question(query):
    patterns = [
        r'ê°œìˆ˜', r'ëª‡\s*ê°œ', r'ëª‡\s*ëª…', r'ì´\s*ê°œ', r'ì´\s*ìˆ˜', 
        r'ì´\s*ì¸ì›', r'ìˆ˜ëŠ”', r'ê°œëŠ”', r'ëª‡\s*ê³³', r'ëª‡\s*íšŒ', 
        r'ì–¼ë§ˆë‚˜', r'ë§ì€', r'ìˆëŠ”', r'ìˆìŠµë‹ˆê¹Œ'
    ]
    return any(re.search(pattern, query) for pattern in patterns)

'''
RAG Pipeline í´ë˜ìŠ¤
ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ PDF íŒŒì¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , í•´ë‹¹ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸
'''
class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file):
        file_hash = get_file_hash(file)
        docs = extract_text_from_pdf(file.name)
        vectorstore = get_vectorstore(docs, file_hash)
        
        # MMR ê²€ìƒ‰ì„ í†µí•´ ë‹¤ì–‘í•œ ì»¨í…ìŠ¤íŠ¸ í™•ë³´
        retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=7, fetch_k=20)
        
        # í…Œì´ë¸” ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš° í…Œì´ë¸” ë°ì´í„°ì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
        if is_count_question(query):
            # í…Œì´ë¸” ë°ì´í„° ë¬¸ì„œ ìš°ì„  ë°°ì¹˜
            table_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" in doc.page_content]
            other_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" not in doc.page_content]
            retrieved_docs = table_docs + other_docs
        
        formatted_context = format_context(retrieved_docs)
        return self.generator(query, formatted_context)

'''
Generator í´ë˜ìŠ¤
Ollama APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ì–´ ì‘ë‹µ ìƒì„±
'''
class Generator:
    def __call__(self, query, context):
        # ìˆ«ì ì§ˆë¬¸ì¸ì§€ í™•ì¸
        count_question = is_count_question(query)
        
        # í”„ë¡¬í”„íŠ¸ ì¡°ì •
        system_prompt = "ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        if count_question:
            system_prompt += " íŠ¹íˆ ìˆ«ìë‚˜ ê°œìˆ˜ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ìˆ«ìë¡œ ë‹µë³€í•˜ê³ , ê³„ì‚° ê³¼ì •ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
        
        formatted_prompt = f"""
        {system_prompt}
        
        ì§ˆë¬¸: {query}

        ì»¨í…ìŠ¤íŠ¸:
        {context}
        
        ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì°¾ì€ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        """
        
        if count_question:
            formatted_prompt += """
            ì´ ì§ˆë¬¸ì€ ê°œìˆ˜ë‚˜ ìˆ«ìë¥¼ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê´€ë ¨ í•­ëª©ë“¤ì„ ì°¾ì•„ ì •í™•íˆ ê³„ì‚°í•´ ì£¼ì„¸ìš”.
            ì˜ˆë¥¼ ë“¤ì–´ 'ì§€ì—­ì´ ë¶€ì‚°ê´‘ì—­ì‹œì¸ ê¸°ì—…ì˜ ìˆ˜'ë¥¼ ë¬»ëŠ”ë‹¤ë©´, í‘œì—ì„œ ì§€ì—­ì´ 'ë¶€ì‚°ê´‘ì—­ì‹œ'ë¡œ í‘œì‹œëœ ê¸°ì—…ë“¤ì˜ ê°œìˆ˜ë¥¼ ì„¸ì–´ ì •í™•í•œ ìˆ«ìë¥¼ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
            """

        response = ollama.chat(
            model='benedict/linkbricks-llama3.1-korean:8b',
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": formatted_prompt}
            ]
        )
        return response['message']['content']

# Gradio ChatInterfaceë¡œ UI ì„¤ì •
generator = Generator()
rag_pipeline = RAGPipeline(generator)

chatbot = gr.ChatInterface(
    fn=lambda msg, hist, file: rag_pipeline(msg, file),
    title="[benedict/linkbricks-llama3.1-korean:8b] Optimized RAG System with Table Support",
    description="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”. ì‹œìŠ¤í…œì´ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³  í‘œ ë°ì´í„°ë¥¼ í¬í•¨í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.",
    additional_inputs=[gr.File(label="ğŸ“„ PDF íŒŒì¼", file_types=[".pdf"])]
)

chatbot.launch()

# import gradio as gr
# import pandas as pd
# import hashlib
# import os
# import re
# import io
# import numpy as np
# import pdfplumber
# import pytesseract
# from PIL import Image
# from langchain.docstore.document import Document
# from langchain.text_splitter import RecursiveCharacterTextSplitter
# from langchain_community.vectorstores import Chroma
# from langchain_ollama import OllamaEmbeddings
# from langchain.document_loaders import PyMuPDFLoader
# import camelot
# import ollama

# # Cache for retrievers
# retriever_cache = {}

# # Embedding model
# embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# # File hash for caching
# def get_file_hash(file):
#     try:
#         file_path = file.name if hasattr(file, "name") else file
#         with open(file_path, "rb") as f:
#             return hashlib.md5(f.read()).hexdigest()
#     except Exception as e:
#         print(f"[ERROR] Ìï¥Ïãú ÏÉùÏÑ± Ïã§Ìå®: {e}")
#         return None

# # OCR + Text + Table extraction from PDF
# def extract_tables_from_pdf(file_path, save_csv_path="auto_extracted_table.csv"):
#     try:
#         tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
#         if len(tables) == 0:
#             tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
#         table_docs = []
#         all_dfs = []
#         for i, table in enumerate(tables):
#             df = table.df
#             all_dfs.append(df)
#             header = [col.strip() for col in df.iloc[0]]
#             for idx, row in df.iloc[1:].iterrows():
#                 row_data = [str(cell).strip() for cell in row]
#                 row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header))])
#                 table_docs.append(Document(page_content=f"[ÌÖåÏù¥Î∏î {i+1}, Ìñâ {idx}]\n{row_text}"))
#         if all_dfs:
#             combined_df = pd.concat(all_dfs, ignore_index=True)
#             combined_df.to_csv(save_csv_path, index=False, header=False)
#             print(f"‚úÖ CSVÎ°ú ÌÖåÏù¥Î∏î Ï†ÄÏû• ÏôÑÎ£å: {save_csv_path}")
#         return table_docs
#     except Exception as e:
#         print(f"[ERROR] ÌÖåÏù¥Î∏î Ï∂îÏ∂ú Ïã§Ìå®: {e}")
#         return []

# def extract_text_from_pdf(file_path):
#     try:
#         loader = PyMuPDFLoader(file_path)
#         docs = loader.load()
#         if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
#             with pdfplumber.open(file_path) as pdf:
#                 for page_num, page in enumerate(pdf.pages):
#                     text = page.extract_text()
#                     if not text or len(text.strip()) < 100:
#                         img = page.to_image()
#                         text = pytesseract.image_to_string(img, lang='kor+eng')
#                     if text:
#                         docs.append(Document(page_content=f"[ÌéòÏù¥ÏßÄ {page_num+1}]\n{text}"))
#         docs.extend(extract_tables_from_pdf(file_path))
#         return docs
#     except Exception as e:
#         print(f"[ERROR] PDF ÌÖçÏä§Ìä∏ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
#         return []

# def split_text(docs):
#     splitter = RecursiveCharacterTextSplitter(
#         chunk_size=1000,
#         chunk_overlap=200,
#         separators=["\n\n", "\n", ". ", " "]
#     )
#     table_docs = [doc for doc in docs if doc.page_content.startswith("[ÌÖåÏù¥Î∏î")]
#     text_docs = [doc for doc in docs if not doc.page_content.startswith("[ÌÖåÏù¥Î∏î")]
#     split_text_docs = splitter.split_documents(text_docs)
#     return split_text_docs + table_docs

# def format_context(retrieved_docs, max_length=3000):
#     context = "\n\n".join([doc.page_content for doc in retrieved_docs])
#     return context[:max_length]

# def is_count_question(query):
#     patterns = [
#         r'Í∞úÏàò', r'Î™á\s*Í∞ú', r'Î™á\s*Î™Ö', r'Ï¥ù\s*Í∞ú', r'Ï¥ù\s*Ïàò', 
#         r'Ï¥ù\s*Ïù∏Ïõê', r'ÏàòÎäî', r'Í∞úÎäî', r'Î™á\s*Í≥≥', r'Î™á\s*Ìöå', 
#         r'ÏñºÎßàÎÇò', r'ÎßéÏùÄ', r'ÏûàÎäî', r'ÏûàÏäµÎãàÍπå'
#     ]
#     return any(re.search(pattern, query) for pattern in patterns)

# class Generator:
#     def __call__(self, query, context):
#         count_q = is_count_question(query)
#         system_prompt = "ÎãπÏã†ÏùÄ Ï†ïÌôïÌïú Î¨∏ÏÑú Î∂ÑÏÑù AI ÎπÑÏÑúÏûÖÎãàÎã§. Ï†úÍ≥µÎêú Ïª®ÌÖçÏä§Ìä∏Î•º Í∏∞Î∞òÏúºÎ°ú Ï†ïÌôïÌïú ÎãµÎ≥ÄÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî."
#         if count_q:
#             system_prompt += " ÌäπÌûà Ïà´ÏûêÎÇò Í∞úÏàòÎ•º Î¨ªÎäî ÏßàÎ¨∏ÏóêÎäî Ï†ïÌôïÌïú Ïà´ÏûêÎ°ú ÎãµÎ≥ÄÌïòÍ≥†, Í≥ÑÏÇ∞ Í≥ºÏ†ïÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî."

#         prompt = f"""
#         {system_prompt}

#         ÏßàÎ¨∏: {query}

#         Ïª®ÌÖçÏä§Ìä∏:
#         {context}

#         Ïª®ÌÖçÏä§Ìä∏ÏóêÏÑú Ï∞æÏùÄ Ï†ïÎ≥¥Î•º Î∞îÌÉïÏúºÎ°ú Î™ÖÌôïÌïòÍ≥† Í∞ÑÍ≤∞ÌïòÍ≤å ÎãµÎ≥ÄÌï¥ Ï£ºÏÑ∏Ïöî.
#         """

#         if count_q:
#             prompt += """
#             Ïù¥ ÏßàÎ¨∏ÏùÄ Í∞úÏàòÎÇò Ïà´ÏûêÎ•º Î¨ªÍ≥† ÏûàÏäµÎãàÎã§. Í¥ÄÎ†® Ìï≠Î™©Îì§ÏùÑ Ï∞æÏïÑ Ï†ïÌôïÌûà Í≥ÑÏÇ∞Ìï¥ Ï£ºÏÑ∏Ïöî.
#             """

#         response = ollama.chat(
#             model='benedict/linkbricks-llama3.1-korean:8b',
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": prompt}
#             ]
#         )
#         return response['message']['content']

# class RAGPipeline:
#     def __init__(self, generator):
#         self.generator = generator

#     def __call__(self, query, file):
#         file_hash = get_file_hash(file)
#         docs = extract_text_from_pdf(file.name)
#         if file_hash in retriever_cache:
#             print("üì¶ Ï∫êÏãúÎêú retriever ÏÇ¨Ïö© Ï§ë...")
#             vectorstore = retriever_cache[file_hash]
#         else:
#             split_docs = split_text(docs)
#             vectorstore = Chroma.from_documents(split_docs, embeddings)
#             retriever_cache[file_hash] = vectorstore

#         retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=7, fetch_k=20)
#         if is_count_question(query):
#             table_docs = [doc for doc in retrieved_docs if "[ÌÖåÏù¥Î∏î" in doc.page_content]
#             other_docs = [doc for doc in retrieved_docs if "[ÌÖåÏù¥Î∏î" not in doc.page_content]
#             retrieved_docs = table_docs + other_docs

#         context = format_context(retrieved_docs)
#         return self.generator(query, context)

# # Gradio Chat Interface
# generator = Generator()
# rag_pipeline = RAGPipeline(generator)

# chatbot = gr.ChatInterface(
#     fn=lambda msg, hist, file: rag_pipeline(msg, file),
#     title="[LLM] PDF Table RAG+TAG ÌÜµÌï© ÏãúÏä§ÌÖú",
#     description="PDF ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† ÏßàÎ¨∏ÌïòÏÑ∏Ïöî. Ìëú Î∞è ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞òÏúºÎ°ú ÎãµÎ≥ÄÌï©ÎãàÎã§.",
#     additional_inputs=[gr.File(label="üìÑ PDF ÌååÏùº", file_types=[".pdf"])]
# )

# chatbot.launch()

import gradio as gr
import hashlib
import os
import re
import pdfplumber
import pytesseract
import pandas as pd
import numpy as np
import camelot
from PIL import Image
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import PyMuPDFLoader
from ChromaDB import build_chroma_db, save_documents_to_csv, CHROMA_DB_DIR
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.document_loaders import PyMuPDFLoader
import ollama

retriever_cache = {}
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

DATASET_DIR = "../dataset"

GLOBAL_VECTORSTORE = Chroma(
    persist_directory = CHROMA_DB_DIR,
    embedding_function = embeddings
)

def get_file_hash(file):
    if file is None:
        raise ValueError("ÏóÖÎ°úÎìúÎêú ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§.")
    try:
        file_path = file.name if hasattr(file, "name") else file
        with open(file_path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] Ìï¥Ïãú ÏÉùÏÑ± Ïã§Ìå®: {e}")
        return None

def extract_tables_from_pdf(file_path):
    try:
        tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
        if len(tables) == 0:
            tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
        table_docs = []
        for i, table in enumerate(tables):
            df = table.df
            header = [col.strip() for col in df.iloc[0]]
            for idx, row in df.iloc[1:].iterrows():
                row_data = [str(cell).strip() for cell in row]
                row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header))])
                table_docs.append(Document(page_content=f"[ÌÖåÏù¥Î∏î {i+1}, Ìñâ {idx}]\n{row_text}"))
        return table_docs
    except Exception as e:
        print(f"[ERROR] ÌÖåÏù¥Î∏î Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        return []

def extract_text_from_pdf(file_path):
    loader = PyMuPDFLoader(file_path)
    docs = loader.load()
    if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
        with pdfplumber.open(file_path) as pdf:
            for page_num, page in enumerate(pdf.pages):
                text = page.extract_text()
                if not text or len(text.strip()) < 100:
                    img = page.to_image().original  # ‚úÖ PIL.ImageÎ°ú Î≥ÄÌôò
                    text = pytesseract.image_to_string(img, lang='kor+eng')
                if text:
                    docs.append(Document(page_content=f"[ÌéòÏù¥ÏßÄ {page_num+1}]\n{text}"))
    docs.extend(extract_tables_from_pdf(file_path))
    return docs

def extract_text_from_csv_xlsx(file_path):
    try:
        ext = os.path.splitext(file_path)[1].lower()
        df = pd.read_csv(file_path) if ext == ".csv" else pd.read_excel(file_path)
        docs = []
        for idx, row in df.iterrows():
            row_text = "\n".join([f"{col}: {row[col]}" for col in df.columns])
            docs.append(Document(page_content=f"[Ìñâ {idx+1}]\n{row_text}"))
        return docs
    except Exception as e:
        print(f"[ERROR] CSV/XLSX Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        return []

def extract_documents(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    if ext == ".pdf":
        return extract_text_from_pdf(file_path)
    elif ext in [".csv", ".xlsx", ".xls"]:
        return extract_text_from_csv_xlsx(file_path)
    return []

def split_text(docs):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    table_docs = [doc for doc in docs if doc.page_content.startswith("[ÌÖåÏù¥Î∏î")]
    text_docs = [doc for doc in docs if not doc.page_content.startswith("[ÌÖåÏù¥Î∏î")]
    return splitter.split_documents(text_docs) + table_docs

def is_count_question(query):
    return any(re.search(p, query) for p in [r'Í∞úÏàò', r'Î™á\s*Í∞ú', r'ÏñºÎßàÎÇò', r'ÏûàÎäî'])

def format_context(docs, max_length=3000):
    return "\n\n".join(doc.page_content for doc in docs)[:max_length]

def load_documents_from_dataset():
    all_documents = []
    for filename in os.listdir(DATASET_DIR):
        if filename.endswith((".pdf", ".csv", ".xlsx", ".xls")):
            file_path = os.path.join(DATASET_DIR, filename)
            print(f"üì• Î¨∏ÏÑú Î°úÎî© Ï§ë: {filename}")
            docs = extract_documents(file_path)
            all_documents.extend(split_text(docs))
    return all_documents

if not os.path.exists(os.path.join(CHROMA_DB_DIR, "index")):
    print("üì¶ Í∏∞Ï°¥ ChromaDB ÏóÜÏùå ‚Üí dataset Ìè¥ÎçîÏóêÏÑú Î¨∏ÏÑú ÏûÑÎ≤†Îî© ÏãúÏûë")
    docs = load_documents_from_dataset()
    if docs:
        print(f"‚úÖ ÏûÑÎ≤†Îî©Ìï† Î¨∏ÏÑú Ïàò: {len(docs)}")
        Chroma.from_documents(
            docs,
            embeddings,
            persist_directory=CHROMA_DB_DIR
        ).persist()
        print("‚úÖ Ï¥àÍ∏∞ Î¨∏ÏÑú ÏûÑÎ≤†Îî© ÏôÑÎ£å")
    else:
        print("‚ö†Ô∏è dataset Ìè¥ÎçîÏóê Î°úÎî©Ìï† Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
else:
    print("‚úÖ ChromaDBÍ∞Ä Ïù¥ÎØ∏ Ï°¥Ïû¨Ìï©ÎãàÎã§. Î¨∏ÏÑú Ïû¨ÏûÑÎ≤†Îî©ÏùÄ ÏÉùÎûµÌï©ÎãàÎã§.")

class Generator:
    def __call__(self, query, context):
        count_q = is_count_question(query)
        prompt = f"""
        ÎãπÏã†ÏùÄ Î¨∏ÏÑúÎ•º Î∂ÑÏÑùÌïòÎäî AIÏûÖÎãàÎã§.
        ÏßàÎ¨∏: {query}
        Ïª®ÌÖçÏä§Ìä∏:
        {context}
        {"Í∞úÏàòÎ•º ÏÑ∏Ïñ¥ Ï†ïÌôïÌïú ÏàòÏπòÎ•º Ï†úÏãúÌï¥Ï£ºÏÑ∏Ïöî." if count_q else ""}
        """
        response = ollama.chat(
            model='benedict/linkbricks-llama3.1-korean:8b',
            messages=[
                {"role": "system", "content": "Ï†ïÌôïÌïú Î¨∏ÏÑú ÏöîÏïΩ"},
                {"role": "user", "content": prompt}
            ]
        )
        return response['message']['content']

class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file=None):
        # if file is None:
        #     return "‚ö†Ô∏è Î®ºÏ†Ä ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌï¥ Ï£ºÏÑ∏Ïöî."
        if file is not None:
            file_hash = get_file_hash(file)
            docs = extract_documents(file.name)
            split_docs = split_text(docs)
            save_documents_to_csv(split_docs, file_hash)
            _ = build_chroma_db(split_docs, file_hash)

        # if file_hash in retriever_cache:
        #     vectorstore = retriever_cache[file_hash]
        # else:
        #     split_docs = split_text(docs)
        #     save_documents_to_csv(split_docs, file_hash)
        #     vectorstore = build_chroma_db(split_docs, file_hash)
        #     retriever_cache[file_hash] = vectorstore

        retrieved_docs = GLOBAL_VECTORSTORE.max_marginal_relevance_search(query, k=7, fetch_k=20)
        print(f"üîç Í≤ÄÏÉâÎêú Î¨∏ÏÑú Ïàò: {len(retrieved_docs)}")
        context = format_context(retrieved_docs)
        print("üìÑ Ï†ÑÎã¨Îêú context ÏùºÎ∂Ä:\n", context[:300])
        return self.generator(query, context)

# Gradio Ïã§Ìñâ
generator = Generator()
rag_pipeline = RAGPipeline(generator)

chatbot = gr.ChatInterface(
    fn=lambda msg, hist, file: rag_pipeline(msg, file),
    title="[LLM] PDF + CSV/XLSX QA",
    description="ÌååÏùº ÏóÖÎ°úÎìú ÌõÑ ÏßàÎ¨∏ÌïòÎ©¥ ÎÇ¥Ïö©ÏùÑ Î∂ÑÏÑùÌï¥ ÎìúÎ†§Ïöî.",
    additional_inputs=[gr.File(label="üìÑ Î¨∏ÏÑú ÏóÖÎ°úÎìú", file_types=[".pdf", ".csv", ".xlsx", ".xls"])]
)
chatbot.launch()

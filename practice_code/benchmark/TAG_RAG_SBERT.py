import gradio as gr
import pandas as pd
import hashlib
import os
import re
import io
import numpy as np
import pdfplumber
import pytesseract
from PIL import Image
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
import camelot
import ollama
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel



# âœ… PEFT ëª¨ë¸ ê²½ë¡œì™€ base ëª¨ë¸
PEFT_MODEL_PATH = "/root/chAtme/chAt-me/practice_code/model/FP32_5epoch/outputs_llama_3_1_5epoch_AI"
BASE_MODEL = "Saxo/Linkbricks-Horizon-AI-Korean-llama-3.1-sft-dpo-8B"

# âœ… ì„ë² ë”© ëª¨ë¸ ë³€ê²½
EMBEDDING_MODEL_NAME = "snunlp/KR-SBERT-Medium-extended-patent2024-hn"
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

# ìºì‹œ
retriever_cache = {}

def get_file_hash(file):
    try:
        file_path = file.name if hasattr(file, "name") else file
        with open(file_path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] í•´ì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def extract_tables_from_pdf(file_path, save_csv_path="auto_extracted_table.csv"):
    try:
        tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
        if len(tables) == 0:
            tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
        table_docs = []
        all_dfs = []
        for i, table in enumerate(tables):
            df = table.df
            all_dfs.append(df)
            header = [col.strip() for col in df.iloc[0]]
            for idx, row in df.iloc[1:].iterrows():
                row_data = [str(cell).strip() for cell in row]
                row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header))])
                table_docs.append(Document(page_content=f"[í…Œì´ë¸” {i+1}, í–‰ {idx}]\n{row_text}"))
        if all_dfs:
            combined_df = pd.concat(all_dfs, ignore_index=True)
            combined_df.to_csv(save_csv_path, index=False, header=False)
            print(f"âœ… CSVë¡œ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {save_csv_path}")
        return table_docs
    except Exception as e:
        print(f"[ERROR] í…Œì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def extract_text_from_pdf(file_path):
    try:
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()
        if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 100:
                        img = page.to_image()
                        text = pytesseract.image_to_string(img, lang='kor+eng')
                    if text:
                        docs.append(Document(page_content=f"[í˜ì´ì§€ {page_num+1}]\n{text}"))
        docs.extend(extract_tables_from_pdf(file_path))
        return docs
    except Exception as e:
        print(f"[ERROR] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def split_text(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " "]
    )
    table_docs = [doc for doc in docs if doc.page_content.startswith("[í…Œì´ë¸”")]
    text_docs = [doc for doc in docs if not doc.page_content.startswith("[í…Œì´ë¸”")]
    split_text_docs = splitter.split_documents(text_docs)
    return split_text_docs + table_docs

def format_context(retrieved_docs, max_length=3000):
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    return context[:max_length]

def is_count_question(query):
    patterns = [
        r'ê°œìˆ˜', r'ëª‡\s*ê°œ', r'ëª‡\s*ëª…', r'ì´\s*ê°œ', r'ì´\s*ìˆ˜', 
        r'ì´\s*ì¸ì›', r'ìˆ˜ëŠ”', r'ê°œëŠ”', r'ëª‡\s*ê³³', r'ëª‡\s*íšŒ', 
        r'ì–¼ë§ˆë‚˜', r'ë§ì€', r'ìˆëŠ”', r'ìˆìŠµë‹ˆê¹Œ'
    ]
    return any(re.search(pattern, query) for pattern in patterns)

class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file):
        file_hash = get_file_hash(file)
        docs = extract_text_from_pdf(file.name)
        if file_hash in retriever_cache:
            print("ğŸ“¦ ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
            vectorstore = retriever_cache[file_hash]
        else:
            split_docs = split_text(docs)
            vectorstore = Chroma.from_documents(split_docs, embeddings)
            retriever_cache[file_hash] = vectorstore

        retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=7, fetch_k=20)
        if is_count_question(query):
            table_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" in doc.page_content]
            other_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" not in doc.page_content]
            retrieved_docs = table_docs + other_docs

        context = format_context(retrieved_docs)
        return self.generator(query, context)

from transformers import BitsAndBytesConfig

class HFGenerator:
    def __init__(self, base_model_path, peft_model_path=None):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

        print(f"ğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘... (device: {self.device})")

        quant_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_enable_fp32_cpu_offload=True
        )

        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            quantization_config=quant_config,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        self.model = PeftModel.from_pretrained(base_model, peft_model_path) if peft_model_path else base_model
        self.model.eval()

    def __call__(self, query, context):
        count_q = is_count_question(query)

        # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ê°œì„ 
        system_prompt = "ì‹œìŠ¤í…œ ë©”ì‹œì§€: ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        if count_q:
            system_prompt += " íŠ¹íˆ ìˆ«ìë‚˜ ê°œìˆ˜ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ìˆ«ìë¡œ ë‹µë³€í•˜ê³ , ê³„ì‚° ê³¼ì •ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."

        prompt = f"""
{system_prompt}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ì»¨í…ìŠ¤íŠ¸:
{context}

--- ë‹µë³€ ì‹œì‘ ---
"""

        if count_q:
            prompt += "\nì´ ì§ˆë¬¸ì€ ê°œìˆ˜ë‚˜ ìˆ«ìë¥¼ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. ê´€ë ¨ í•­ëª©ë“¤ì„ ì°¾ì•„ ì •í™•íˆ ê³„ì‚°í•´ ì£¼ì„¸ìš”."

        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,  # ğŸ” ë°˜ë³µ ì–µì œ
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

        # âœ… 'ë‹µë³€:' ë°˜ë³µ ì œê±°
        response_text = re.sub(r'(ë‹µë³€[:ï¼š]?\s*){2,}', 'ë‹µë³€: ', response_text)

        # âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¬¸ì¥ ì œê±°
        if "ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤" in response_text:
            response_text = response_text.split("ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤")[-1].strip()

        # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ë¶„ì ì œê±°
        response_text = response_text.replace("--- ë‹µë³€ ì‹œì‘ ---", "").strip()

        # âœ… ì¶”ê°€ ì¢…ë£Œ í† í° ë°©ì–´
        for stop_seq in ["ì‹œìŠ¤í…œ ë©”ì‹œì§€:", "ì‚¬ìš©ì ì§ˆë¬¸:", "ì»¨í…ìŠ¤íŠ¸:", "---", "###"]:
            if stop_seq in response_text:
                response_text = response_text.split(stop_seq)[0].strip()

        return response_text



# Gradio UI
generator = HFGenerator(BASE_MODEL, PEFT_MODEL_PATH)
rag_pipeline = RAGPipeline(generator)

chatbot = gr.ChatInterface(
    fn=lambda msg, hist, file: rag_pipeline(msg, file),
    title="[LLM] PDF Table RAG+TAG í†µí•© ì‹œìŠ¤í…œ",
    description="PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”. í‘œ ë° í…ìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤.",
    additional_inputs=[gr.File(label="ğŸ“„ PDF íŒŒì¼", file_types=[".pdf"])]
)

if __name__ == "__main__":
    chatbot.launch()

import gradio as gr
import pandas as pd
import hashlib
import os
import re
import io
import numpy as np
import pdfplumber
import pytesseract
from PIL import Image
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
from langchain.document_loaders import PyMuPDFLoader
import camelot
import ollama
import sqlite3
from datetime import datetime

# Cache for retrievers and DataFrames
retriever_cache = {}
dataframe_cache = {}

# Embedding model
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# File hash for caching
def get_file_hash(file):
    try:
        file_path = file.name if hasattr(file, "name") else file
        with open(file_path, "rb") as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] í•´ì‹œ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# SQL ì¿¼ë¦¬ íŒ¨í„´ ê°ì§€
def is_sql_query(query):
    sql_patterns = [
        r'select\s+.*\s+from',
        r'count\(\*\)',
        r'where\s+.*\s+like',
        r'group\s+by',
        r'order\s+by',
        r'ë¶€ì‚°.*ëª‡\s*ê°œ',
        r'ì„œìš¸.*ëª‡\s*ê°œ',
        r'ê´‘ì£¼.*ëª‡\s*ê°œ',
        r'ê²½ìƒ.*ëª‡\s*ê°œ',
        r'ê²½ë‚¨.*ëª‡\s*ê°œ',
        r'ê²½ë¶.*ëª‡\s*ê°œ',
        r'ì¸ì²œ.*ëª‡\s*ê°œ',
        r'ì œì£¼.*ëª‡\s*ê°œ',
        r'ê°•ì›.*ëª‡\s*ê°œ',
        r'ëŒ€ì „.*ëª‡\s*ê°œ',
        r'ê²½ê¸°.*ëª‡\s*ê°œ',
        r'ì¶©ë‚¨.*ëª‡\s*ê°œ',
        r'ì¶©ë¶.*ëª‡\s*ê°œ',
        r'ì¶©ì²­.*ëª‡\s*ê°œ',
        r'ëŒ€êµ¬.*ëª‡\s*ê°œ',
        r'ì „ë¼.*ëª‡\s*ê°œ',
        r'ì „ë‚¨.*ëª‡\s*ê°œ',
        r'ì „ë¶.*ëª‡\s*ê°œ',
        r'ìš¸ì‚°.*ëª‡\s*ê°œ',
        r'.*ì§€ì—­.*ê°œìˆ˜',
        r'.*ì§€ì—­.*ëª‡'
    ]
    query_lower = query.lower()
    return any(re.search(pattern, query_lower) for pattern in sql_patterns)

# ìì—°ì–´ë¥¼ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜
def convert_to_sql(query):
    query_lower = query.lower()
    
    if 'ë¶€ì‚°' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ë¶€ì‚°%'"
    
    if 'ì„œìš¸' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì„œìš¸%'"
    
    if 'ê´‘ì£¼' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê´‘ì£¼%'"
    
    if ('ê²½ë‚¨' in query or 'ê²½ìƒë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ìƒë‚¨ë„%'"
    
    if ('ê²½ë¶' in query or 'ê²½ìƒë¶ë„' in query) and ('ëª‡'in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ìƒë¶ë„%'"

    if ('ê²½ìƒ' in query or 'ê²½ìƒë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ê²½ìƒë‚¨ë„%' OR ì§€ì—­ LIKE '%ê²½ìƒë¶ë„%')"

    if 'ì¸ì²œ' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¸ì²œ%'"

    if ('ì œì£¼' in query or 'ì œì£¼ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì œì£¼%'"

    if ('ê°•ì›' in query or 'ê°•ì›ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê°•ì›%'"

    if 'ëŒ€ì „' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ëŒ€ì „%'"

    if 'ê²½ê¸°' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ê²½ê¸°%'"

    if ('ì¶©ë‚¨' in query or 'ì¶©ì²­ë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¶©ì²­ë‚¨ë„%'"

    if ('ì¶©ë¶' in query or 'ì¶©ì²­ë¶ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì¶©ì²­ë¶ë„%'"

    if 'ì¶©ì²­' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ì¶©ì²­ë‚¨ë„%' OR ì§€ì—­ LIKE '%ì¶©ì²­ë¶ë„%')"

    if 'ëŒ€êµ¬' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ëŒ€êµ¬ê´‘ì—­ì‹œ%'"

    if 'ì „ë¼ë„' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE (ì§€ì—­ LIKE '%ì „ë¼ë‚¨ë„%' OR ì§€ì—­ LIKE '%ì „ë¼ë¶ë„%')"

    if ('ì „ë‚¨' in query or 'ì „ë¼ë‚¨ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì „ë¼ë‚¨ë„%'"

    if ('ì „ë¶' in query or 'ì „ë¼ë¶ë„' in query) and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ì „ë¼ë¶ë„%'"

    if 'ìš¸ì‚°' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query or 'count' in query_lower):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%ìš¸ì‚°ê´‘ì—­ì‹œ%'"
    
    if 'ë¶€ì‚°' in query and ('ì„¤ë¦½ì—°ë„' in query or 'íšŒì‚¬ì„¤ë¦½ì¼' in query or 'ì—°ë„ë³„' in query or 'ë…„ë„ë³„' in query):
        return "SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, count(*) FROM companies WHERE ì§€ì—­ LIKE '%ë¶€ì‚°%' GROUP BY ì„¤ë¦½ì—°ë„ ORDER BY ì„¤ë¦½ì—°ë„;"
    
    if 'ì„œìš¸' in query and ('ì„¤ë¦½ì—°ë„' in query or 'íšŒì‚¬ì„¤ë¦½ì¼' in query or 'ì—°ë„ë³„' in query or 'ë…„ë„ë³„' in query):
        return "SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, count(*) FROM companies WHERE ì§€ì—­ LIKE '%ì„œìš¸%' GROUP BY ì„¤ë¦½ì—°ë„ ORDER BY ì„¤ë¦½ì—°ë„;"
    
    if ('ì „ì²´' in query or 'ëª¨ë“  ì§€ì—­' in query or 'ì§€ì—­ë³„' in query) and ('ì—°ë„ë³„' in query or 'ì„¤ë¦½ì—°ë„' in query):
        return """
        SELECT SUBSTR(íšŒì‚¬ì„¤ë¦½ì¼, 1, 4) AS ì„¤ë¦½ì—°ë„, COUNT(*) AS ê¸°ì—…ìˆ˜ 
        FROM companies 
        GROUP BY ì„¤ë¦½ì—°ë„ 
        ORDER BY ì„¤ë¦½ì—°ë„
        """
    
    # ì§€ì—­ë³„ ê°œìˆ˜ ì¿¼ë¦¬
    region_match = re.search(r'(\w+ê´‘ì—­ì‹œ|\w+ì‹œ|\w+ë„|\w+íŠ¹ë³„ì‹œ).*(?:ëª‡|ê°œìˆ˜|count)', query)
    if region_match:
        region = region_match.group(1)
        return f"SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì§€ì—­ LIKE '%{region}%'"
    
    # ì „ë¬¸ë¶„ì•¼ë³„ ê°œìˆ˜ ì¿¼ë¦¬
    if 'ì‹œê°ì§€ëŠ¥' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ê¸°ì—…ìˆ˜ FROM companies WHERE ì „ë¬¸ë¶„ì•¼ LIKE '%ì‹œê°ì§€ëŠ¥%'"
    
    # ê¸°ë³¸ ì „ì²´ ê°œìˆ˜
    if 'ì „ì²´' in query and ('ëª‡' in query or 'ê°œìˆ˜' in query):
        return "SELECT COUNT(*) as ì´ê¸°ì—…ìˆ˜ FROM companies"
    
    return None

# DataFrameì„ SQLite DBë¡œ ë³€í™˜
def create_sqlite_from_dataframe(df, db_path="temp_companies.db"):
    try:
        conn = sqlite3.connect(db_path)
        df.to_sql('companies', conn, if_exists='replace', index=False)
        conn.close()
        return True
    except Exception as e:
        print(f"[ERROR] SQLite DB ìƒì„± ì‹¤íŒ¨: {e}")
        return False

# SQL ì¿¼ë¦¬ ì‹¤í–‰
def execute_sql_query(sql_query, db_path="temp_companies.db"):
    try:
        conn = sqlite3.connect(db_path)
        result = pd.read_sql_query(sql_query, conn)
        conn.close()
        return result
    except Exception as e:
        print(f"[ERROR] SQL ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None

# OCR + Text + Table extraction from PDF
def extract_tables_from_pdf(file_path, save_csv_path="auto_extracted_table.csv"):
    try:
        tables = camelot.read_pdf(file_path, pages='all', flavor='lattice')
        if len(tables) == 0:
            tables = camelot.read_pdf(file_path, pages='all', flavor='stream')
        
        table_docs = []
        all_dfs = []
        
        for i, table in enumerate(tables):
            df = table.df
            all_dfs.append(df)
            
            # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„ ë° í—¤ë” ì„¤ì •
            if len(df) > 0:
                # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
                header = [str(col).strip() for col in df.iloc[0]]
                df.columns = header
                df = df.iloc[1:].reset_index(drop=True)
                
                # ê° í–‰ì„ ë¬¸ì„œë¡œ ë³€í™˜
                for idx, row in df.iterrows():
                    row_data = [str(cell).strip() for cell in row]
                    row_text = "\n".join([f"{header[i]}: {row_data[i]}" for i in range(len(header)) if i < len(row_data)])
                    table_docs.append(Document(page_content=f"[í…Œì´ë¸” {i+1}, í–‰ {idx+1}]\n{row_text}"))
        
        # ëª¨ë“  í…Œì´ë¸”ì„ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ê²°í•©
        if all_dfs:
            # ì²« ë²ˆì§¸ í…Œì´ë¸”ì˜ í—¤ë”ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
            main_df = all_dfs[0].copy()
            if len(main_df) > 0:
                header = [str(col).strip() for col in main_df.iloc[0]]
                main_df.columns = header
                main_df = main_df.iloc[1:].reset_index(drop=True)
                
                # ë‚˜ë¨¸ì§€ í…Œì´ë¸”ë“¤ ì¶”ê°€
                for df in all_dfs[1:]:
                    if len(df) > 0:
                        df.columns = header[:len(df.columns)]
                        df = df.iloc[1:].reset_index(drop=True)
                        main_df = pd.concat([main_df, df], ignore_index=True)
                
                # CSV ì €ì¥
                main_df.to_csv(save_csv_path, index=False, encoding='utf-8-sig')
                print(f"âœ… CSVë¡œ í…Œì´ë¸” ì €ì¥ ì™„ë£Œ: {save_csv_path}")
                
                # DataFrame ìºì‹œì— ì €ì¥
                dataframe_cache['companies'] = main_df
                
                # SQLite DB ìƒì„±
                create_sqlite_from_dataframe(main_df)
                
        return table_docs
    except Exception as e:
        print(f"[ERROR] í…Œì´ë¸” ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def extract_text_from_pdf(file_path):
    try:
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ í™•ì¸ ë° OCR ë³´ì™„
        if not docs or any(len(doc.page_content.strip()) < 100 for doc in docs):
            print("ğŸ“„ OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘...")
            with pdfplumber.open(file_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    text = page.extract_text()
                    if not text or len(text.strip()) < 100:
                        img = page.to_image()
                        text = pytesseract.image_to_string(img, lang='kor+eng')
                    if text:
                        docs.append(Document(page_content=f"[í˜ì´ì§€ {page_num+1}]\n{text}"))
        
        # í…Œì´ë¸” ì¶”ì¶œ ë° ì¶”ê°€
        table_docs = extract_tables_from_pdf(file_path)
        docs.extend(table_docs)
        
        return docs
    except Exception as e:
        print(f"[ERROR] PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

def split_text(docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " "]
    )
    
    # í…Œì´ë¸”ê³¼ í…ìŠ¤íŠ¸ ë¬¸ì„œ ë¶„ë¦¬
    table_docs = [doc for doc in docs if doc.page_content.startswith("[í…Œì´ë¸”")]
    text_docs = [doc for doc in docs if not doc.page_content.startswith("[í…Œì´ë¸”")]
    
    # í…ìŠ¤íŠ¸ ë¬¸ì„œë§Œ ë¶„í• 
    split_text_docs = splitter.split_documents(text_docs)
    
    return split_text_docs + table_docs

def format_context(retrieved_docs, max_length=3000):
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    return context[:max_length]

def is_count_question(query):
    patterns = [
        r'ê°œìˆ˜', r'ëª‡\s*ê°œ', r'ëª‡\s*ëª…', r'ì´\s*ê°œ', r'ì´\s*ìˆ˜', 
        r'ì´\s*ì¸ì›', r'ìˆ˜ëŠ”', r'ê°œëŠ”', r'ëª‡\s*ê³³', r'ëª‡\s*íšŒ', 
        r'ì–¼ë§ˆë‚˜', r'ë§ì€', r'ìˆëŠ”', r'ìˆìŠµë‹ˆê¹Œ'
    ]
    return any(re.search(pattern, query) for pattern in patterns)

class Generator:
    def __call__(self, query, context, sql_result=None):
        count_q = is_count_question(query)
        system_prompt = "ë‹¹ì‹ ì€ ì •í™•í•œ ë¬¸ì„œ ë¶„ì„ AI ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."
        
        if count_q:
            system_prompt += " íŠ¹íˆ ìˆ«ìë‚˜ ê°œìˆ˜ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì—ëŠ” ì •í™•í•œ ìˆ«ìë¡œ ë‹µë³€í•˜ê³ , ê³„ì‚° ê³¼ì •ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."

        prompt = f"""
        {system_prompt}

        ì§ˆë¬¸: {query}

        ì»¨í…ìŠ¤íŠ¸:
        {context}
        """
        
        # SQL ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if sql_result is not None:
            prompt += f"""
            
        SQL ì¿¼ë¦¬ ê²°ê³¼:
        {sql_result.to_string()}
        
        ìœ„ì˜ SQL ì¿¼ë¦¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ ì£¼ì„¸ìš”.
        """

        if count_q:
            prompt += """
            
        ì´ ì§ˆë¬¸ì€ ê°œìˆ˜ë‚˜ ìˆ«ìë¥¼ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. SQL ê²°ê³¼ë‚˜ ê´€ë ¨ í•­ëª©ë“¤ì„ ì°¾ì•„ ì •í™•íˆ ê³„ì‚°í•´ ì£¼ì„¸ìš”.
        """

        try:
            response = ollama.chat(
                model='benedict/linkbricks-llama3.1-korean:8b',
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ]
            )
            return response['message']['content']
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file):
        if not file:
            return "PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
        
        file_hash = get_file_hash(file)
        
        # SQL ì¿¼ë¦¬ ê°ì§€ ë° ì²˜ë¦¬
        if is_sql_query(query):
            print("ğŸ” SQL ì¿¼ë¦¬ ê°ì§€ë¨")
            
            # ìºì‹œëœ DataFrameì´ ì—†ìœ¼ë©´ PDFì—ì„œ ì¶”ì¶œ
            if 'companies' not in dataframe_cache:
                print("ğŸ“Š PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                docs = extract_text_from_pdf(file.name)
            
            # SQL ì¿¼ë¦¬ ìƒì„± ë° ì‹¤í–‰
            sql_query = convert_to_sql(query)
            if sql_query:
                print(f"ğŸ“ ìƒì„±ëœ SQL: {sql_query}")
                sql_result = execute_sql_query(sql_query)
                
                if sql_result is not None:
                    print(f"âœ… SQL ê²°ê³¼: {sql_result}")
                    return f"SQL ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼:\n\n{sql_result.to_string()}\n\nì§ˆë¬¸í•˜ì‹  ë‚´ìš©ì— ëŒ€í•œ ë‹µë³€: {sql_result.iloc[0, 0]}ê°œ"
                else:
                    return "SQL ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            else:
                return "í•´ë‹¹ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤."
        
        # ì¼ë°˜ RAG ê²€ìƒ‰
        print("ğŸ” ì¼ë°˜ RAG ê²€ìƒ‰ ì‹¤í–‰")
        
        # ìºì‹œëœ retriever í™•ì¸
        if file_hash in retriever_cache:
            print("ğŸ“¦ ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
            vectorstore = retriever_cache[file_hash]
        else:
            print("ğŸ”„ ìƒˆë¡œìš´ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            docs = extract_text_from_pdf(file.name)
            split_docs = split_text(docs)
            vectorstore = Chroma.from_documents(split_docs, embeddings)
            retriever_cache[file_hash] = vectorstore

        # ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = vectorstore.max_marginal_relevance_search(query, k=7, fetch_k=20)
        
        # ê°œìˆ˜ ì§ˆë¬¸ì˜ ê²½ìš° í…Œì´ë¸” ìš°ì„  ì²˜ë¦¬
        if is_count_question(query):
            table_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" in doc.page_content]
            other_docs = [doc for doc in retrieved_docs if "[í…Œì´ë¸”" not in doc.page_content]
            retrieved_docs = table_docs + other_docs

        context = format_context(retrieved_docs)
        return self.generator(query, context)

# Gradio Chat Interface
generator = Generator()
rag_pipeline = RAGPipeline(generator)

def chat_interface(message, history, file):
    try:
        response = rag_pipeline(message, file)
        return response
    except Exception as e:
        return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
chatbot = gr.ChatInterface(
    fn=chat_interface,
    title="[LLM] PDF Table RAG+SQL í†µí•© ì‹œìŠ¤í…œ",
    description="""
    ğŸ“Š PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•˜ì„¸ìš”. 
    
    ğŸ’¡ **ì˜ˆì‹œ ì§ˆë¬¸:**
    - "ë¶€ì‚°ê´‘ì—­ì‹œì— ìœ„ì¹˜í•œ ê¸°ì—…ì˜ ìˆ˜ëŠ” ëª‡ê°œì•¼?"
    - "ì‹œê°ì§€ëŠ¥ ë¶„ì•¼ ê¸°ì—…ì´ ëª‡ê°œì•¼?"
    - "ì „ì²´ ê¸°ì—… ìˆ˜ëŠ”?"
    """,
    additional_inputs=[
        gr.File(
            label="ğŸ“„ PDF íŒŒì¼", 
            file_types=[".pdf"],
            type="filepath"
        )
    ]
)

if __name__ == "__main__":
    chatbot.launch(share=True, debug=True)

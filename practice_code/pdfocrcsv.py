import gradio as gr
import pandas as pd
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ollama import OllamaEmbeddings
import ollama
import pdfplumber
import pytesseract
from PIL import Image
import hashlib
import os

# ìºì‹œ ì €ì¥ì†Œ
retriever_cache = {}

# íŒŒì¼ í•´ì‹œ ìƒì„± í•¨ìˆ˜
def get_file_hash(file):
    file_path = file.name if hasattr(file, "name") else file
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

# OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_with_ocr(page):
    text = page.extract_text()
    if not text:
        image = page.to_image()
        text = pytesseract.image_to_string(image)
    return text

# PDFì—ì„œ í‘œ ì¶”ì¶œ í›„ CSV ì €ì¥
def extract_tables_from_pdf(file):
    markdown_tables = []
    merged_tables = []
    try:
        with pdfplumber.open(file) as pdf:
            for i, page in enumerate(pdf.pages):
                try:
                    tables = page.extract_tables()
                    if not tables:
                        continue
                    for table in tables:
                        df = pd.DataFrame(table)
                        df.columns = df.iloc[0]
                        df = df[1:]
                        merged_tables.append(df)
                        markdown_tables.append(df.to_markdown(index=False))
                except Exception as e:
                    print(f"í…Œì´ë¸” ì¶”ì¶œ ì—ëŸ¬ (í˜ì´ì§€ {i + 1}): {e}")
    except Exception as e:
        print(f"PDF ì—´ê¸° ì‹¤íŒ¨: {e}")
        return ""

    if merged_tables:
        result = pd.concat(merged_tables, ignore_index=True)
        result.to_csv("tables.csv", index=False)
        print(f"tables.csv ì €ì¥ ì™„ë£Œ (ì´ {len(merged_tables)}ê°œ í…Œì´ë¸”)")
        return "\n\n".join(markdown_tables)
    else:
        print("í…Œì´ë¸”ì´ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return ""

# PDF ë¡œë“œ ë° ë²¡í„°í™” + í…Œì´ë¸” ì¶”ì¶œ
def load_and_retrieve_docs(file):
    file_hash = get_file_hash(file)

    if file_hash in retriever_cache:
        print("ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
        return retriever_cache[file_hash]

    text = ""
    try:
        with pdfplumber.open(file) as pdf:
            for page in pdf.pages:
                page_text = extract_text_with_ocr(page)
                if page_text:
                    text += page_text
    except Exception as e:
        return f"PDF ì½ê¸° ì˜¤ë¥˜: {e}"

    if not text:
        return "PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    # ë²¡í„°í™”
    docs = [Document(page_content=text)]
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)
    embeddings = OllamaEmbeddings(model="mxbai-embed-large")
    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    retriever_cache[file_hash] = retriever
    return retriever

# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG ê¸°ë°˜ QA
def rag_chain(message, history, file):
    # í…Œì´ë¸” ì¶”ì¶œ ë° ì €ì¥
    table_markdown = extract_tables_from_pdf(file)

    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ìš© ë²¡í„°í™”
    retriever = load_and_retrieve_docs(file)
    if isinstance(retriever, str):  # ì—ëŸ¬ ë¬¸ìì—´ ë°˜í™˜ëœ ê²½ìš°
        return retriever

    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = retriever.invoke(message)
    formatted_context = format_docs(retrieved_docs)

    # LLM ì§ˆë¬¸ í¬ë§·
    formatted_prompt = f"""
Question: {message}

Context: {formatted_context}

Tables:\n{table_markdown}
"""

    response = ollama.chat(
        model='llama3.2',
        messages=[
            {"role": "system", "content": "You are a helpful assistant. Use the given PDF content and tables to answer the question."},
            {"role": "user", "content": formatted_prompt}
        ]
    )

    summary = response['message']['content']
    save_to_csv(summary)
    return summary

# ìš”ì•½ ê²°ê³¼ ì €ì¥
def save_to_csv(summary):
    df = pd.DataFrame({"Summary": [summary]})
    df.to_csv("summary.csv", index=False)

# Gradio ì¸í„°í˜ì´ìŠ¤ êµ¬ì„±
chatbot = gr.ChatInterface(
    fn=rag_chain,
    title="[LLAMA 3.2] RAG + Table ì¶”ì¶œ ì±—ë´‡ ì‹œìŠ¤í…œ",
    description="PDFíŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (íŒŒì¼ì€ ìºì‹œì— ì €ì¥ë˜ê³ , í…Œì´ë¸”ì€ tables.csvë¡œ ì €ì¥ë©ë‹ˆë‹¤.)",
    additional_inputs=[gr.File(label="ğŸ“„ PDF íŒŒì¼", file_types=[".pdf"])]
)

chatbot.launch()

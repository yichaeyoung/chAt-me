import gradio as gr
import pandas as pd
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.vectorstores import FAISS
from langchain_ollama import OllamaEmbeddings
import ollama
import pdfplumber
import pytesseract
from PIL import Image
import hashlib
import os
from langchain.document_loaders import PyMuPDFLoader

# ìºì‹œ ì €ì¥ì†Œ (LRU ë°©ì‹)
retriever_cache = {}
# CACHE_LIMIT = 5

# íŒŒì¼ í•´ì‹œ ìƒì„± í•¨ìˆ˜
def get_file_hash(file):
    file_path = file.name if hasattr(file, "name") else file
    with open(file_path, "rb") as f:
        return hashlib.md5(f.read()).hexdigest()

''' 
PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDFLoader + OCR ê²°í•©)
pdfplumberë³´ë‹¤ PyMuPDFLoaderê°€ ë” ë¹ ë¥´ê³  ì•ˆì •ì ì¸ pdf íŒŒì‹±ì´ ê°€ëŠ¥
PyMuPDFLoaderë¥¼ ìš°ì„  ì‚¬ìš©í›„ í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” í˜ì´ì§€ì—ë§Œ pytesseractë¡œ OCR ì ìš©í•˜ë„ë¡ ìµœì í™”
'''
def extract_text_from_pdf(file_path):
    loader = PyMuPDFLoader(file_path)
    docs = loader.load()
    if not docs:
        with pdfplumber.open(file_path) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or pytesseract.image_to_string(page.to_image())
                docs.append(Document(page_content=text))
    return docs

''' 
í…ìŠ¤íŠ¸ ë¶„í•  í•¨ìˆ˜
" ", ". " ë“±ì„ ë„£ì–´ì„œ ë” ì„¸ë°€í•˜ê²Œ ë¶„í• 
'''
def split_text(docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " "]
    )
    return text_splitter.split_documents(docs)

'''
ì»¨í…ìŠ¤íŠ¸ í¬ë§¤íŒ… í•¨ìˆ˜
ì¤‘ìš” ë¬¸ì¥ ìš°ì„  í¬í•¨ ë° ê¸¸ì´ ì œí•œ
'''
def format_context(retrieved_docs, max_length=3000):
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    return context[:max_length]

embeddings = OllamaEmbeddings(model="mxbai-embed-large")

'''
ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ìºì‹œ ê´€ë¦¬
ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì´ê¸° ìœ„í•´ì„œ LRU ë°©ì‹ìœ¼ë¡œ ìºì‹œ ê´€ë¦¬ ê¸°ëŠ¥ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ.(í˜„ì¬ëŠ” ì£¼ì„ì²˜ë¦¬)
'''
def get_vectorstore(docs, file_hash):
    if file_hash in retriever_cache:
        print("ğŸ“¦ ìºì‹œëœ retriever ì‚¬ìš© ì¤‘...")
        return retriever_cache[file_hash]

    split_docs = split_text(docs)
    vectorstore = Chroma.from_documents(split_docs, embeddings)
    # if len(retriever_cache) >= CACHE_LIMIT:
    #     retriever_cache.pop(next(iter(retriever_cache)))  # ê°€ì¥ ì˜¤ë˜ëœ ìºì‹œ ì œê±°
    retriever_cache[file_hash] = vectorstore
    return vectorstore

'''
RAG Pipeline í´ë˜ìŠ¤
ì£¼ì–´ì§„ ì¿¼ë¦¬ì— ëŒ€í•´ PDF íŒŒì¼ì—ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , í•´ë‹¹ ì»¨í…ìŠ¤íŠ¸ì˜¤ í•¨ê»˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸
'''
class RAGPipeline:
    def __init__(self, generator):
        self.generator = generator

    def __call__(self, query, file):
        file_hash = get_file_hash(file)
        docs = extract_text_from_pdf(file.name)
        vectorstore = get_vectorstore(docs, file_hash)
        retrieved_docs = vectorstore.similarity_search(query, k=5) # 3ì—ì„œ 5ë¡œ ë°”ê¿ˆ.
        # retrieved_docs = vectorstore.similarity_search_with_score(query, k=5, search_type='mmr')
        formatted_context = format_context(retrieved_docs)
        return self.generator(query, formatted_context)

'''
Generator í´ë˜ìŠ¤
Ollama APIë¥¼ í˜¸ì¶œí•˜ì—¬ ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìì—°ì–´ ì‘ë‹µ ìƒì„±
'''
class Generator:
    def __call__(self, query, context):
        formatted_prompt = f"""
        You are a highly accurate document analysis assistant.
        Your task is to provide a precise answer to the user's question based on the provided context.

        Question: {query}

        Context:
        {context}

        Answer with specific and concise information from the context.
        """
        response = ollama.chat(
            model='benedict/linkbricks-llama3.1-korean:8b',
            messages=[
                {"role": "system", "content": "You are a helpful assistant. Analyze the PDF content and answer the question."},
                {"role": "user", "content": formatted_prompt}
            ]
        )
        return response['message']['content']

# Gradio ChatInterfaceë¡œ UI ì„¤ì •
generator = Generator()
rag_pipeline = RAGPipeline(generator)

chatbot = gr.ChatInterface(
    fn=lambda msg, hist, file: rag_pipeline(msg, file),
    title="[benedict/linkbricks-llama3.1-korean:8b] Optimized RAG System",
    description="Upload a PDF file and ask questions. The system retrieves relevant context and generates responses.",
    additional_inputs=[gr.File(label="ğŸ“„ PDF íŒŒì¼", file_types=[".pdf"])]
)

chatbot.launch()
